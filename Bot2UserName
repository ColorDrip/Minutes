import os
import pandas as pd
import numpy as np
import re
import random
from datetime import datetime
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk import everygrams
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Download required NLTK resources
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
    nltk.data.find('corpora/wordnet')
except:
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('wordnet')

class NLTKFAQSystem:
    def __init__(self, faq_path="faq_knowledge.xlsx", log_path="conversation_log.xlsx"):
        self.faq_path = faq_path
        self.log_path = log_path
        
        # Initialize NLTK components
        self.stop_words = set(stopwords.words('english'))
        self.lemmatizer = WordNetLemmatizer()
        
        # Initialize files
        self.initialize_files()
        
        # Load data
        self.faqs = pd.read_excel(self.faq_path)
        self.logs = pd.read_excel(self.log_path) if os.path.exists(self.log_path) else pd.DataFrame()
        
        # Prepare vectorizer with n-grams
        self.vectorizer = TfidfVectorizer(
            tokenizer=self.nltk_tokenizer,
            ngram_range=(1, 2),  # Unigrams and bigrams
            stop_words=list(self.stop_words)
        )
        self.question_vectors = self.vectorizer.fit_transform(self.faqs['Question'])
    
    def initialize_files(self):
        """Create template files if they don't exist"""
        # FAQ knowledge base
        if not os.path.exists(self.faq_path) or pd.read_excel(self.faq_path).empty:
            faq_template = pd.DataFrame({
                'Question': [
                    'What is your return policy?',
                    'How do I reset my password?',
                    'Do you ship internationally?',
                    'How can I track my order?',
                    'What payment methods do you accept?'
                ],
                'Answer': [
                    'We offer a 30-day return policy on all unused items',
                    'Click "Forgot Password" on the login page',
                    'Yes, we ship to over 50 countries',
                    'Use the tracking number sent to your email',
                    'We accept credit cards, PayPal, and bank transfers'
                ],
                'Category': ['Policy', 'Account', 'Shipping', 'Orders', 'Payments'],
                'Keywords': [
                    'return, refund, exchange', 
                    'password, login, access', 
                    'ship, international, global',
                    'track, package, delivery',
                    'payment, credit card, paypal'
                ],
                'Last Used': [np.nan] * 5,
                'Use Count': [0] * 5
            })
            faq_template.to_excel(self.faq_path, index=False)
            print(f"âœ… Created new FAQ knowledge base at: {self.faq_path}")
        
        # Conversation log
        if not os.path.exists(self.log_path):
            log_template = pd.DataFrame(columns=[
                'Timestamp', 'User Input', 'System Response', 
                'Matched Question', 'Confidence'
            ])
            log_template.to_excel(self.log_path, index=False)
            print(f"âœ… Created new conversation log at: {self.log_path}")
    
    def nltk_tokenizer(self, text):
        """Custom tokenizer using NLTK with lemmatization and n-grams"""
        # Clean and tokenize
        text = re.sub(r'[^\w\s]', '', text.lower())
        tokens = word_tokenize(text)
        
        # Remove stopwords and lemmatize
        filtered_tokens = [
            self.lemmatizer.lemmatize(token) 
            for token in tokens 
            if token not in self.stop_words and len(token) > 2
        ]
        
        # Generate unigrams and bigrams
        ngrams = list(everygrams(filtered_tokens, max_len=2))
        return ['_'.join(gram) for gram in ngrams]
    
    def find_best_match(self, query):
        """Find best match using TF-IDF with n-grams"""
        # Vectorize query
        query_vec = self.vectorizer.transform([query])
        
        # Calculate cosine similarity
        similarities = cosine_similarity(query_vec, self.question_vectors)
        max_index = np.argmax(similarities)
        max_similarity = similarities[0, max_index]
        
        if max_similarity > 0.3:  # Similarity threshold
            return self.faqs.iloc[max_index], max_similarity
        return None, max_similarity
    
    def update_faq_usage(self, index):
        """Update usage statistics for a FAQ entry"""
        self.faqs.at[index, 'Use Count'] += 1
        self.faqs.at[index, 'Last Used'] = datetime.now()
        self.faqs.to_excel(self.faq_path, index=False)
    
    def log_conversation(self, user_input, response, matched_question, confidence):
        """Record conversation to Excel log"""
        new_entry = pd.DataFrame([{
            'Timestamp': datetime.now(),
            'User Input': user_input,
            'System Response': response,
            'Matched Question': matched_question,
            'Confidence': confidence
        }])
        
        # Append to existing log
        if not self.logs.empty:
            updated_log = pd.concat([self.logs, new_entry], ignore_index=True)
        else:
            updated_log = new_entry
            
        updated_log.to_excel(self.log_path, index=False)
        self.logs = updated_log
    
    def extract_keywords(self, text):
        """Extract keywords using NLTK"""
        tokens = self.nltk_tokenizer(text)
        return set(tokens)  # Return unique tokens
    
    def generate_fallback_response(self, query):
        """Generate intelligent fallback using NLTK"""
        # Extract keywords from query
        keywords = self.extract_keywords(query)
        
        # Find related questions based on keywords
        related_questions = []
        for _, row in self.faqs.iterrows():
            faq_keywords = set(row['Keywords'].lower().split(', '))
            if keywords & faq_keywords:
                related_questions.append(row['Question'])
        
        # Format response
        if related_questions:
            response = "I couldn't find an exact match, but these might help:\n"
            for i, q in enumerate(set(related_questions[:3]), 1):
                response += f"{i}. {q}\n"
        else:
            top_questions = self.faqs.sort_values('Use Count', ascending=False)['Question'].head(3).tolist()
            response = "I'm not sure about that. Try these popular questions:\n"
            for i, q in enumerate(top_questions, 1):
                response += f"{i}. {q}\n"
        
        return response
    
    def run(self):
        """Main interaction loop"""
        print("\nðŸ§  NLTK-Powered FAQ System (Unigrams + Bigrams)")
        print(f"ðŸ“š Knowledge base: {len(self.faqs)} questions loaded")
        print("Type 'exit' to quit\n")
        
        while True:
            user_input = input("You: ").strip()
            if not user_input:
                continue
                
            if user_input.lower() == 'exit':
                print("\nThank you for using the FAQ system!")
                break
                
            # Find best match
            match, confidence = self.find_best_match(user_input)
            
            if match is not None:
                # Update usage stats
                self.update_faq_usage(match.name)
                
                # Format response
                response = (
                    f"ðŸ¤– [Confidence: {confidence:.0%}]\n"
                    f"Q: {match['Question']}\n"
                    f"A: {match['Answer']}\n"
                    f"Category: {match['Category']} | Used: {match['Use Count']} times"
                )
                print(f"\n{response}")
                
                # Log conversation
                self.log_conversation(
                    user_input, 
                    response, 
                    match['Question'], 
                    confidence
                )
            else:
                response = self.generate_fallback_response(user_input)
                print(f"\nðŸ¤– {response}")
                self.log_conversation(user_input, response, None, confidence)


if __name__ == "__main__":
    # Get file paths from user or use defaults
    faq_path = input("Enter FAQ file path (default: faq_knowledge.xlsx): ").strip() or "faq_knowledge.xlsx"
    log_path = input("Enter log file path (default: conversation_log.xlsx): ").strip() or "conversation_log.xlsx"
    
    # Initialize and run system
    faq_system = NLTKFAQSystem(faq_path, log_path)
    faq_system.run()