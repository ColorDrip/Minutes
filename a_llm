# Step 0: Install required libraries
# Run in terminal:
# pip install -q pypdf faiss-cpu sentence-transformers transformers langchain accelerate

# Step 1: Import libraries
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
import torch

# Step 2: PDF Processing
def process_pdf(pdf_path):
    """Extract text from PDF and split into chunks"""
    loader = PyPDFLoader(pdf_path)
    pages = loader.load()
    
    # Configure text splitter
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        length_function=len,
        separators=["\n\n", "\n", ". ", "! ", "? ", "; ", ", ", " "]
    )
    
    # Split documents into chunks
    chunks = text_splitter.split_documents(pages)
    print(f"Created {len(chunks)} chunks from {len(pages)} pages")
    return chunks

# Step 3: Create Vector Database
def create_vector_db(chunks, db_name="pdf_vector_index"):
    """Generate embeddings and store in FAISS vector database"""
    # Use efficient small embedding model
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    
    # Create vector store
    vector_db = FAISS.from_documents(chunks, embeddings)
    
    # Save locally for future use
    vector_db.save_local(db_name)
    print(f"Vector database saved as '{db_name}'")
    return vector_db, embeddings

# Step 4: Load Question Answering Model
def load_qa_model():
    """Initialize small LLM for question answering"""
    model_name = "google/flan-t5-small"  # Efficient 80M parameter model
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    
    # Create text generation pipeline
    qa_pipeline = pipeline(
        "text2text-generation",
        model=model,
        tokenizer=tokenizer,
        max_length=200,
        device=0 if torch.cuda.is_available() else -1  # Use GPU if available
    )
    return qa_pipeline

# Step 5: Answer Questions from PDF
def answer_from_pdf(question, vector_db, embeddings, qa_pipeline, k=3):
    """Retrieve relevant context and generate answer"""
    # Load vector database if needed
    if isinstance(vector_db, str):
        vector_db = FAISS.load_local(vector_db, embeddings)
    
    # Find most relevant chunks
    relevant_docs = vector_db.similarity_search(question, k=k)
    context = "\n".join([doc.page_content for doc in relevant_docs])
    
    # Construct optimized prompt
    prompt = f"""
    Answer the question using ONLY the context below.
    If you don't know the answer, say "I couldn't find relevant information".
    
    Context:
    {context}
    
    Question: {question}
    Answer:
    """
    
    # Generate answer
    result = qa_pipeline(prompt)
    return result[0]['generated_text'].strip()

# Step 6: Main Execution Flow
if __name__ == "__main__":
    # Configuration
    PDF_PATH = "your_document.pdf"  # <-- Replace with your PDF path
    DB_NAME = "pdf_qa_index"
    
    # Run pipeline
    print("Processing PDF...")
    chunks = process_pdf(PDF_PATH)
    
    print("\nCreating vector database...")
    vector_db, embeddings = create_vector_db(chunks, DB_NAME)
    
    print("\nLoading QA model...")
    qa_model = load_qa_model()
    
    # Interactive Q&A loop
    print("\nReady for questions! (type 'exit' to quit)")
    while True:
        query = input("\nYour question: ")
        if query.lower() == 'exit':
            break
            
        answer = answer_from_pdf(query, vector_db, embeddings, qa_model)
        print(f"\nAnswer: {answer}")