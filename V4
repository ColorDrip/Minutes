import re
from spellchecker import SpellChecker
import spacy
from gensim.summarization import summarize
from transformers import pipeline
import numpy as np

# Initialize components
nlp = spacy.load("en_core_web_sm")
spell = SpellChecker()
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Configuration
INPUT_FILE = "chat_log.txt"
CLEANED_FILE = "cleaned_chat.txt"
SUMMARY_FILE = "summary.txt"
CUSTOM_WORDS = ["lol", "brb", "omg", "btw"]  # Add chat-specific slang

class ChatCleaner:
    def __init__(self):
        self.spell = SpellChecker()
        self.spell.word_frequency.load_words(CUSTOM_WORDS)
        
    def clean_text(self, text):
        # Remove timestamps and special characters
        text = re.sub(r'\b\d{1,2}:\d{2}\b', '', text)  # Remove timestamps
        text = re.sub(r'[^\w\s\'-]', '', text)  # Remove special chars
        
        # Fix common chat abbreviations
        replacements = {
            'u ': 'you ',
            'r ': 'are ',
            ' ur ': ' your ',
            ' n ': ' and ',
        }
        for k, v in replacements.items():
            text = text.replace(k, v)
            
        return text.strip()
    
    def correct_spelling(self, text):
        words = text.split()
        corrected = []
        for word in words:
            # Preserve capitalization for proper nouns
            is_capitalized = word.istitle()
            correction = self.spell.correction(word) or word
            if is_capitalized and correction:
                correction = correction.capitalize()
            corrected.append(correction)
        return ' '.join(corrected)
    
    def context_sensitive_correction(self, text):
        # Use spaCy for better context understanding
        doc = nlp(text)
        corrected = []
        for token in doc:
            if token.text.lower() in self.spell.unknown([token.text]):
                suggestion = self.spell.correction(token.text)
                if suggestion and suggestion != token.text:
                    corrected.append(suggestion)
                else:
                    corrected.append(token.text)
            else:
                corrected.append(token.text)
        return ' '.join(corrected)
    
    def process_file(self):
        unique_lines = set()
        with open(INPUT_FILE, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
        cleaned_lines = []
        for line in lines:
            line = self.clean_text(line)
            line = self.correct_spelling(line)
            line = self.context_sensitive_correction(line)
            if line and line not in unique_lines:
                cleaned_lines.append(line)
                unique_lines.add(line)
        
        # Save cleaned chat
        with open(CLEANED_FILE, 'w', encoding='utf-8') as f:
            f.write('\n'.join(cleaned_lines))
            
        return cleaned_lines
    
class ChatAnalyzer:
    def __init__(self, text):
        self.full_text = ' '.join(text)
        
    def generate_summary(self):
        # Abstractive summarization using BART
        if len(self.full_text.split()) > 100:
            return summarizer(self.full_text, max_length=150, min_length=30, do_sample=False)[0]['summary_text']
        # Extractive summarization for short texts
        return summarize(self.full_text, ratio=0.2)
    
    def analyze_conversation(self):
        doc = nlp(self.full_text)
        # Extract key information
        entities = [(ent.text, ent.label_) for ent in doc.ents]
        topics = list(set([token.lemma_ for token in doc if token.pos_ in ['NOUN', 'PROPN']]))
        return {
            'entities': entities,
            'main_topics': topics[:5],
            'summary': self.generate_summary()
        }

if __name__ == "__main__":
    # Step 1: Clean and deduplicate chat
    cleaner = ChatCleaner()
    cleaned_chat = cleaner.process_file()
    
    # Step 2: Analyze and summarize
    analyzer = ChatAnalyzer(cleaned_chat)
    analysis = analyzer.analyze_conversation()
    
    # Save summary
    with open(SUMMARY_FILE, 'w', encoding='utf-8') as f:
        f.write("Chat Summary:\n")
        f.write(analysis['summary'] + "\n\n")
        f.write("Key Entities:\n")
        for entity, label in analysis['entities']:
            f.write(f"- {entity} ({label})\n")
            
    print(f"Cleaned chat saved to {CLEANED_FILE}")
    print(f"Summary saved to {SUMMARY_FILE}")
